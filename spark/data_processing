# ---------------------------------------------------------
# üöÄ IMPORTS
# ---------------------------------------------------------
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import col
from pyspark.sql.types import StringType
from pyspark.ml import Pipeline
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, CountVectorizer, VectorAssembler
from pyspark.ml.clustering import LDA
from pyspark.ml.regression import RandomForestRegressor
from transformers import pipeline

# ---------------------------------------------------------
# ‚ö° SPARK SESSION
# ---------------------------------------------------------
spark = SparkSession.builder \
    .appName("Reddit_Stream_ML_Pipeline") \
    .getOrCreate()

# ---------------------------------------------------------
# 1Ô∏è‚É£ DEFINIR LA FONCTION mapPartitions POUR SENTIMENT
# ---------------------------------------------------------
def sentiment_partition(iterator):
    """
    Applique DistilBERT sentiment sur chaque partition.
    Le mod√®le est charg√© UNE seule fois par partition.
    """
    sentiment_pipeline = pipeline(
        "sentiment-analysis",
        model="distilbert-base-uncased-finetuned-sst-2-english"
    )
    for row in iterator:
        text = row['text'][:512]  # max 512 tokens
        sentiment = sentiment_pipeline(text)[0]['label']
        yield Row(
            id=row['id'],
            subreddit=row['subreddit'],
            text=row['text'],
            score=row['score'],  # score r√©el
            sentiment=sentiment
        )

# ---------------------------------------------------------
# 2Ô∏è‚É£ DEFINIR LE PIPELINE ML POUR PREPROCESSING
# ---------------------------------------------------------
tokenizer = RegexTokenizer(inputCol="text", outputCol="words", pattern="\\W")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol="filtered", outputCol="word_vectors")
cv = CountVectorizer(inputCol="filtered", outputCol="features_lda", vocabSize=1000, minDF=1)

pipeline_prep = Pipeline(stages=[tokenizer, remover, word2Vec, cv])

# LDA pour topic modeling
lda = LDA(k=3, maxIter=5, featuresCol="features_lda")

# VectorAssembler pour RandomForest
assembler = VectorAssembler(inputCols=["word_vectors"], outputCol="features_regression")

# RandomForestRegressor (exemple pour d√©monstration)
rf = RandomForestRegressor(featuresCol="features_regression", labelCol="score")

# ---------------------------------------------------------
# 3Ô∏è‚É£ FONCTION PROCESS_BATCH POUR CHAQUE MICRO-BATCH
# ---------------------------------------------------------
def process_batch(batch_df, batch_id):
    if batch_df.count() == 0:
        return

    print(f"=== Traitement du batch {batch_id} ===")
    
    # A. Sentiment Analysis (mapPartitions)
    df_sent = batch_df.rdd.mapPartitions(sentiment_partition).toDF()
    
    # B. Pipeline ML pour preprocessing
    model_prep = pipeline_prep.fit(df_sent)
    df_transformed = model_prep.transform(df_sent)
    
    # C. Topic Modeling (LDA)
    lda_model = lda.fit(df_transformed)
    df_topics = lda_model.transform(df_transformed)
    
    # D. Regression (RandomForest pour pr√©diction d'engagement)
    df_features = assembler.transform(df_topics)
    rf_model = rf.fit(df_features)
    df_predictions = rf_model.transform(df_features)
    
    # E. Affichage / stockage
    final_output = df_predictions.select(
        "id", "subreddit", "text", "sentiment",
        "topicDistribution", "score", "prediction"
    )
    
    final_output.show(truncate=False)
    
    # Optionnel : sauvegarde vers MongoDB / Parquet / ElasticSearch
    # final_output.write.format("parquet").mode("append").save("/path/to/output")
    
    # Lib√©rer la m√©moire
    batch_df.unpersist()

# ---------------------------------------------------------
# 4Ô∏è‚É£ LECTURE DU STREAM (Kafka ou autre source)
# ---------------------------------------------------------
# Exemple Kafka
clean_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "Reddit_Data") \
    .option("startingOffsets", "latest") \
    .load() \
    .selectExpr("CAST(value AS STRING) as json") \
    .select("json.*")  # adapter selon ton JSON

# ---------------------------------------------------------
# 5Ô∏è‚É£ D√âMARRER LE STREAM
# ---------------------------------------------------------
query = clean_df.writeStream \
    .outputMode("append") \
    .foreachBatch(process_batch) \
    .start()

query.awaitTermination()
