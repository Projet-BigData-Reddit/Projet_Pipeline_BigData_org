import os
# Fichier: reddit_to_kafka_pipeline.py
import praw
import time
import json
import re
from kafka.admin import KafkaAdminClient, NewTopic
from kafka import KafkaProducer
from kafka.errors import TopicAlreadyExistsError, NoBrokersAvailable
from config import CLIENT_ID,CLIENT_SECRET,USER_AGENT,USERNAME,KAFKA_TOPIC,KAFKA_BROKER_URL,PARTITIONS,REPLICATION_FACTOR

# =============================================================================
# ---                      PARTIE 1: CONFIGURATION                          ---
# =============================================================================

KAFKA_TOPIC = KAFKA_TOPIC




# =============================================================================

def create_kafka_topic():
    """
    Se connecte √† Kafka et s'assure que le topic existe.
    C'est la premi√®re √©tape avant de lancer le producteur.
    """
    print("--- √âtape 1: V√©rification du topic Kafka ---")
    try:
        admin_client = KafkaAdminClient(
            bootstrap_servers=KAFKA_BROKER_URL,
            client_id='topic_setup_client'
        )
        print("‚úÖ Connexion admin √† Kafka r√©ussie !")

        topic = NewTopic(
            name=KAFKA_TOPIC,
            num_partitions=PARTITIONS,
            replication_factor=REPLICATION_FACTOR
        )
        admin_client.create_topics(new_topics=[topic], validate_only=False)
        print(f"üéâ Topic '{KAFKA_TOPIC}' cr√©√© avec succ√®s !")
        admin_client.close()

    except TopicAlreadyExistsError:
        print(f"üëç Le topic '{KAFKA_TOPIC}' existe d√©j√†. Aucune action n'est n√©cessaire.")
        admin_client.close()
    except NoBrokersAvailable:
        print(f"‚ùå ERREUR : Impossible de se connecter au broker Kafka √† l'adresse {KAFKA_BROKER_URL}.")
        print("   Veuillez vous assurer que le conteneur Docker Kafka est bien en cours d'ex√©cution.")
        return False
    except Exception as e:
        print(f"Une erreur inattendue est survenue lors de la cr√©ation du topic : {e}")
        return False
    
    return True

def create_kafka_producer():
    """
    Cr√©e et retourne un producteur Kafka, pr√™t √† envoyer des messages.
    """
    print("\n--- √âtape 2: D√©marrage du producteur Kafka ---")
    try:
        producer = KafkaProducer(
            bootstrap_servers=KAFKA_BROKER_URL,
            # S√©rialise les messages en format JSON puis les encode en bytes
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        print("‚úÖ Producteur Kafka connect√© et pr√™t !")
        return producer
    except NoBrokersAvailable:
        print("‚ùå ERREUR : Impossible de cr√©er le producteur. Broker non disponible.")
        return None
    except Exception as e:
        print(f"Une erreur inattendue est survenue lors de la cr√©ation du producteur : {e}")
        return None


# ---------- Connexion √† Reddit ----------
reddit = praw.Reddit(
    client_id=CLIENT_ID,
    client_secret=CLIENT_SECRET,
    user_agent=USER_AGENT
)

# ---------- Subreddits et mots-cl√©s ----------
subreddits = "cryptocurrency+Bitcoin+Ethereum+altcoin+CryptoMarkets+ethtrader+CryptoTechnology+CryptoCurrencyNews+DeFi+CryptoMoonShots+Dogecoin+Cardano+Solana+ShibaInu"
keywords = [
    "crypto", "cryptocurrency", "bitcoin", "btc", "ethereum", "eth",
    "blockchain", "altcoin", "token", "defi", "nft", "smart contract",
    "mining", "miner", "hash rate", "wallet", "hardware wallet", "cold storage",
    "staking", "yield farming", "airdrop", "ico", "ido", "web3",
    "dogecoin", "doge", "cardano", "ada", "solana", "sol", "shiba inu", "shib"
]
keywords = [k.lower() for k in keywords]

# ---------- Fichier de sortie ----------

output_file = "reddit_crypto_data.json"
if not os.path.exists(output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump([], f)

# ---------- Fonctions utilitaires ----------
def is_relevant(text):
    """V√©rifie si le commentaire contient un mot-cl√© pertinent"""
    if not text:
        return False
    text = text.lower()
    return any(re.search(r'\b{}\b'.format(re.escape(k)), text) for k in keywords)


def save_comment(data):
    """Sauvegarde un commentaire unique dans le fichier JSON"""
    try:
        # cr√©er fichier vide si absent
        if not os.path.exists(output_file):
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump([], f)

        with open(output_file, "r+", encoding="utf-8") as f:
            try:
                comments = json.load(f)
            except json.JSONDecodeError:
                comments = []  # si le JSON est corrompu

            # √©viter les doublons via l'ID du commentaire
            if not any(c["id"] == data["id"] for c in comments):
                comments.append(data)

            # r√©√©criture propre du fichier
            f.seek(0)
            json.dump(comments, f, ensure_ascii=False, indent=2)
            f.truncate()

    except Exception as e:
        print("Erreur sauvegarde:", e)


# ---------- R√©cup√©rer anciens commentaires ----------
def fetch_old_comments(limit=1000):
    print("üì• R√©cup√©ration des anciens commentaires...")
    try:
        subreddit = reddit.subreddit(subreddits)
        for comment in subreddit.comments(limit=limit):
            if is_relevant(comment.body):
                data = {
                    "id": comment.id,
                    "author": str(comment.author),
                    "subreddit": str(comment.subreddit),
                    "text": comment.body,
                    "timestamp": comment.created_utc,
                    "score": comment.score,
                    "num_replies": len(comment.replies)
                }
                print(f"[Ancien] {data['text']}")
                save_comment(data)
        print("‚úÖ R√©cup√©ration des anciens commentaires termin√©e.")
    except Exception as e:
        print("‚ö†Ô∏è Erreur r√©cup√©ration anciens commentaires:", e)


# ---------- Stream en temps r√©el ----------
def stream_new_comments():
    print("üì° √âcoute des nouveaux commentaires en temps r√©el...")
    try:
        for comment in reddit.subreddit(subreddits).stream.comments(skip_existing=True):
            try:
                if is_relevant(comment.body):
                    data = {
                        "id": comment.id,
                        "author": str(comment.author),
                        "subreddit": str(comment.subreddit),
                        "text": comment.body,
                        "timestamp": comment.created_utc,
                        "score": comment.score,
                        "num_replies": len(comment.replies)
                    }

                    print(f"[Nouveau] {data['text']}")
                    save_comment(data)
            except Exception as e:
                print("Erreur traitement commentaire:", e)
                time.sleep(1)
    except KeyboardInterrupt:
        print("üõë Arr√™t du stream Reddit (Ctrl+C).")
    except Exception as e:
        print("‚ö†Ô∏è Erreur g√©n√©rale stream:", e)
        time.sleep(5)


# ---------- Main ----------
if __name__ == "__main__":
    fetch_old_comments(limit=500)   # R√©cup√®re les 500 derniers commentaires pertinents
    stream_new_comments()           # Puis √©coute en temps r√©el

